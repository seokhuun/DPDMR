{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3a68eb1-3694-4a6c-b15b-35b7a543f326",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T00:04:41.786650Z",
     "iopub.status.busy": "2024-08-02T00:04:41.786650Z",
     "iopub.status.idle": "2024-08-02T00:05:48.018706Z",
     "shell.execute_reply": "2024-08-02T00:05:48.017705Z",
     "shell.execute_reply.started": "2024-08-02T00:04:41.786650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n",
      "\n",
      "Testing with memory size: 200\n",
      "\n",
      "=== Class 0 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\seokhun\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for optimal n of clusters...\n",
      "Optimal n of clusters for class 0: 9 (score: 0.6154)\n",
      "Set: n_clusters: 9, n_neighbors: 5 (Memory allocated for class 0: 9*5=200)\n",
      "Epoch 1/10 Loss: 1.2656\n",
      "Epoch 10/10 Loss: 0.5590\n",
      "Test set: Average loss: 0.7778, Accuracy: 0.5897, Recall: 0.5897, Precision: 0.7550\n",
      "Intermediate Test set: Average loss: 0.7778, Accuracy: 0.5897, Recall: 0.5897, Precision: 0.7550\n",
      "\n",
      "=== Class 1 ===\n",
      "Accuracy on class 0: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\seokhun\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for optimal n of clusters...\n",
      "Optimal n of clusters for class 1: 10 (score: 0.5372)\n",
      "Set: n_clusters: 10, n_neighbors: 5 (Memory allocated for class 1: 10*5=200)\n",
      "Epoch 1/10 Loss: 0.9786\n",
      "Epoch 10/10 Loss: 0.8997\n",
      "Performance on previous classes after class 1: Loss: 0.4492, Accuracy: 0.9550\n",
      "Test set: Average loss: 0.5780, Accuracy: 0.8342, Recall: 0.8342, Precision: 0.8601\n",
      "Intermediate Test set: Average loss: 0.5780, Accuracy: 0.8342, Recall: 0.8342, Precision: 0.8601\n",
      "\n",
      "=== Class 2 ===\n",
      "Accuracy on class 0: 0.9760\n",
      "Accuracy on class 1: 0.9956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\seokhun\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for optimal n of clusters...\n",
      "Optimal n of clusters for class 2: 9 (score: 0.6146)\n",
      "Set: n_clusters: 9, n_neighbors: 5 (Memory allocated for class 2: 9*5=200)\n",
      "Epoch 1/10 Loss: 0.7553\n",
      "Epoch 10/10 Loss: 0.5519\n",
      "Performance on previous classes after class 2: Loss: 0.3966, Accuracy: 0.8700\n",
      "Test set: Average loss: 0.5818, Accuracy: 0.7174, Recall: 0.7174, Precision: 0.8254\n",
      "Intermediate Test set: Average loss: 0.5818, Accuracy: 0.7174, Recall: 0.7174, Precision: 0.8254\n",
      "\n",
      "=== Class 3 ===\n",
      "Accuracy on class 0: 0.9978\n",
      "Accuracy on class 1: 0.9434\n",
      "Accuracy on class 2: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\seokhun\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for optimal n of clusters...\n",
      "Optimal n of clusters for class 3: 5 (score: 0.5731)\n",
      "Set: n_clusters: 5, n_neighbors: 10 (Memory allocated for class 3: 5*10=200)\n",
      "Epoch 1/10 Loss: 0.8285\n",
      "Epoch 10/10 Loss: 0.8966\n",
      "Performance on previous classes after class 3: Loss: 0.3796, Accuracy: 0.9600\n",
      "Test set: Average loss: 0.4220, Accuracy: 0.9348, Recall: 0.9348, Precision: 0.9393\n",
      "Intermediate Test set: Average loss: 0.4220, Accuracy: 0.9348, Recall: 0.9348, Precision: 0.9393\n",
      "\n",
      "Test set: Average loss: 0.0138, Accuracy: 0.9348, Recall: 0.9348, Precision: 0.9393\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, confusion_matrix, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, log_loss, matthews_corrcoef, cohen_kappa_score, balanced_accuracy_score, \n",
    "    jaccard_score, fowlkes_mallows_score, hamming_loss, zero_one_loss, classification_report,\n",
    "    roc_curve, auc, precision_recall_curve\n",
    ")\n",
    "import logging\n",
    "\n",
    "# 로깅 설정\n",
    "logging.basicConfig(filename='training.log', level=logging.INFO, format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class PRPDDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, max_length=44):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.max_length = max_length\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.label_map = self._load_data()\n",
    "    \n",
    "    def _load_data(self):\n",
    "        label_map = {label: idx for idx, label in enumerate(os.listdir(self.root_dir))}\n",
    "        for label in os.listdir(self.root_dir):\n",
    "            label_dir = os.path.join(self.root_dir, label)\n",
    "            if os.path.isdir(label_dir):\n",
    "                for file in os.listdir(label_dir):\n",
    "                    if file.endswith('.wav'):  # 파일 확장자 확인\n",
    "                        file_path = os.path.join(label_dir, file)\n",
    "                        features = self.extract_features(file_path)\n",
    "                        self.data.append(features)\n",
    "                        self.labels.append(label_map[label])\n",
    "        return label_map\n",
    "\n",
    "    def extract_features(self, file_path, sr=96000, n_fft=1024, hop_length=512):\n",
    "        audio_data, sample_rate = librosa.load(file_path, sr=sr)\n",
    "        stft = librosa.stft(audio_data, n_fft=n_fft, hop_length=hop_length)\n",
    "        spectrogram = np.abs(stft)\n",
    "        log_spectrogram = librosa.amplitude_to_db(spectrogram)\n",
    "        if log_spectrogram.shape[1] > self.max_length:\n",
    "            log_spectrogram = log_spectrogram[:, :self.max_length]\n",
    "        else:\n",
    "            log_spectrogram = np.pad(log_spectrogram, ((0, 0), (0, self.max_length - log_spectrogram.shape[1])), mode='constant')\n",
    "        return torch.tensor(log_spectrogram.flatten(), dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Prototypical Network 정의\n",
    "class PrototypicalNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(PrototypicalNetwork, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Classifier 정의\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Triplet Loss 정의\n",
    "def triplet_loss(anchor, positive, negative, margin=1.0):\n",
    "    pos_dist = torch.sum((anchor - positive) ** 2, dim=1)\n",
    "    neg_dist = torch.sum((anchor - negative) ** 2, dim=1)\n",
    "    loss = torch.clamp(pos_dist - neg_dist + margin, min=0.0)\n",
    "    return loss.mean()\n",
    "\n",
    "# Triplet 샘플 생성 함수\n",
    "def create_triplets(x, y):\n",
    "    anchors, positives, negatives = [], [], []\n",
    "    for i in range(len(y)):\n",
    "        anchor = x[i]\n",
    "        positive_idx = (y == y[i]).nonzero(as_tuple=True)[0].tolist()\n",
    "        negative_idx = (y != y[i]).nonzero(as_tuple=True)[0].tolist()\n",
    "        positive_idx.remove(i)\n",
    "        if not positive_idx or not negative_idx:\n",
    "            continue\n",
    "        positive = x[positive_idx[torch.randint(len(positive_idx), (1,))]]\n",
    "        negative = x[negative_idx[torch.randint(len(negative_idx), (1,))]]\n",
    "        anchors.append(anchor)\n",
    "        positives.append(positive)\n",
    "        negatives.append(negative)\n",
    "    return torch.stack(anchors), torch.stack(positives), torch.stack(negatives)\n",
    "\n",
    "# MetaLearner 정의\n",
    "class MetaLearner:\n",
    "    def __init__(self, proto_net, classifier, inner_lr=0.01, outer_lr=0.001, weight_decay=1e-4, device='cpu'):\n",
    "        self.proto_net = proto_net\n",
    "        self.classifier = classifier\n",
    "        self.inner_optimizer = optim.SGD(self.classifier.parameters(), lr=inner_lr, weight_decay=weight_decay)\n",
    "        self.outer_optimizer = optim.Adam(\n",
    "            list(self.proto_net.parameters()) + list(self.classifier.parameters()), lr=outer_lr, weight_decay=weight_decay\n",
    "        )\n",
    "        self.device = device\n",
    "    \n",
    "    def inner_update(self, support_set):\n",
    "        self.classifier.train()\n",
    "        self.proto_net.eval()\n",
    "        support_set = (support_set[0].to(self.device), support_set[1].to(self.device))\n",
    "        classification_loss = self.compute_loss(support_set)\n",
    "        prototypical_loss = self.compute_prototypical_loss(support_set)\n",
    "        triplet_loss_value = self.compute_triplet_loss(support_set)\n",
    "        loss = classification_loss + 0.5 * prototypical_loss + 0.5 * triplet_loss_value  # 가중치 조정\n",
    "        self.inner_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.inner_optimizer.step()\n",
    "        return loss.item()\n",
    "    \n",
    "    def compute_loss(self, data):\n",
    "        x, y = data\n",
    "        x, y = x.to(self.device), y.to(self.device)\n",
    "        embeddings = self.proto_net(x)\n",
    "        logits = self.classifier(embeddings)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        return loss\n",
    "    \n",
    "    def compute_prototypical_loss(self, support_set):\n",
    "        x, y = support_set\n",
    "        x, y = x.to(self.device), y.to(self.device)\n",
    "        unique_labels = y.unique()\n",
    "        if len(unique_labels) == 0:\n",
    "            return torch.tensor(0.0, requires_grad=True, device=self.device)\n",
    "        prototypes = []\n",
    "        for class_label in unique_labels:\n",
    "            class_indices = (y == class_label).nonzero(as_tuple=True)[0]\n",
    "            if len(class_indices) > 0:\n",
    "                class_prototypes = self.proto_net(x[class_indices]).mean(0)\n",
    "                prototypes.append(class_prototypes)\n",
    "        if len(prototypes) == 0:\n",
    "            return torch.tensor(0.0, requires_grad=True, device=self.device)\n",
    "        prototypes = torch.stack(prototypes)\n",
    "        prototypical_loss = 0\n",
    "        for idx in range(x.size(0)):\n",
    "            distances = torch.cdist(self.proto_net(x[idx].unsqueeze(0)), prototypes)\n",
    "            target = torch.where(unique_labels == y[idx])[0].item()\n",
    "            prototypical_loss += F.cross_entropy(-distances, torch.tensor([target], device=self.device))\n",
    "        return prototypical_loss\n",
    "    \n",
    "    def compute_triplet_loss(self, data):\n",
    "        x, y = data\n",
    "        x, y = x.to(self.device), y.to(self.device)\n",
    "        anchors, positives, negatives = create_triplets(x, y)\n",
    "        anchor_embeddings = self.proto_net(anchors)\n",
    "        positive_embeddings = self.proto_net(positives)\n",
    "        negative_embeddings = self.proto_net(negatives)\n",
    "        return triplet_loss(anchor_embeddings, positive_embeddings, negative_embeddings)\n",
    "    \n",
    "    def outer_update(self, query_set, memory):\n",
    "        self.proto_net.train()\n",
    "        self.classifier.train()\n",
    "        query_set = (query_set[0].to(self.device), query_set[1].to(self.device))\n",
    "        classification_loss = self.compute_loss(query_set)\n",
    "        prototypical_loss = self.compute_prototypical_loss(query_set)\n",
    "        triplet_loss_value = self.compute_triplet_loss(query_set)\n",
    "        query_loss = classification_loss + prototypical_loss + triplet_loss_value\n",
    "        if memory is not None:\n",
    "            memory = (memory[0].to(self.device), memory[1].to(self.device))\n",
    "            memory_classification_loss = self.compute_loss(memory)\n",
    "            memory_prototypical_loss = self.compute_prototypical_loss(memory)\n",
    "            memory_triplet_loss_value = self.compute_triplet_loss(memory)\n",
    "            memory_loss = memory_classification_loss + memory_prototypical_loss + memory_triplet_loss_value\n",
    "            total_loss = query_loss + memory_loss\n",
    "        else:\n",
    "            total_loss = query_loss\n",
    "        self.outer_optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.outer_optimizer.step()\n",
    "        return total_loss.item()\n",
    "\n",
    "# StaticMemoryReplay 클래스 정의\n",
    "class StaticMemoryReplay:\n",
    "    def __init__(self, memory_data, memory_labels):\n",
    "        self.memory_data = memory_data\n",
    "        self.memory_labels = memory_labels\n",
    "\n",
    "    def sample_memory(self):\n",
    "        return self.memory_data, self.memory_labels\n",
    "\n",
    "# 성능 지표 기록 함수 수정\n",
    "def log_performance_metrics(epoch, y_true, y_pred, y_pred_proba, loss, classes, metrics_dict):\n",
    "    y_pred = np.array(y_pred).flatten()  # Flatten the prediction array\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_pred_proba, multi_class='ovr', average='weighted')\n",
    "    logloss = log_loss(y_true, y_pred_proba)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    jaccard = jaccard_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    fmi = fowlkes_mallows_score(y_true, y_pred)\n",
    "    hamming = hamming_loss(y_true, y_pred)\n",
    "    zero_one = zero_one_loss(y_true, y_pred)\n",
    "\n",
    "    metrics = {\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'AUC': auc,\n",
    "        'Log Loss': logloss,\n",
    "        'MCC': mcc,\n",
    "        'Cohen\\'s Kappa': kappa,\n",
    "        'Balanced Accuracy': balanced_acc,\n",
    "        'Jaccard Index': jaccard,\n",
    "        'FMI': fmi,\n",
    "        'Hamming Loss': hamming,\n",
    "        'Zero-One Loss': zero_one\n",
    "    }\n",
    "\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        if metric_name not in metrics_dict:\n",
    "            metrics_dict[metric_name] = []\n",
    "        metrics_dict[metric_name].append(metric_value)\n",
    "\n",
    "    logger.info(f'Epoch {epoch}: Loss: {loss:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, '\n",
    "                f'F1-Score: {f1:.4f}, AUC: {auc:.4f}, Log Loss: {logloss:.4f}, MCC: {mcc:.4f}, '\n",
    "                f'Cohen\\'s Kappa: {kappa:.4f}, Balanced Accuracy: {balanced_acc:.4f}, '\n",
    "                f'Jaccard Index: {jaccard:.4f}, FMI: {fmi:.4f}, Hamming Loss: {hamming:.4f}, '\n",
    "                f'Zero-One Loss: {zero_one:.4f}')\n",
    "\n",
    "# 혼동 행렬 기록 함수 정의\n",
    "def log_confusion_matrix(cm, classes):\n",
    "    logger.info(f'Confusion Matrix: {cm}')\n",
    "\n",
    "# ROC 및 PR 곡선 계산 함수 정의\n",
    "def compute_roc_pr_curves(all_targets, all_pred_proba, classes):\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    pr_auc = dict()\n",
    "    n_classes = len(classes)\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(np.array(all_targets) == i, all_pred_proba[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        precision[i], recall[i], _ = precision_recall_curve(np.array(all_targets) == i, all_pred_proba[:, i])\n",
    "        pr_auc[i] = auc(recall[i], precision[i])\n",
    "\n",
    "    logger.info(f'ROC AUC Scores: {roc_auc}')\n",
    "    logger.info(f'PR AUC Scores: {pr_auc}')\n",
    "\n",
    "# 테스트 함수 정의 (정확도와 리콜을 출력하도록 수정)\n",
    "def test(meta_learner, test_loader, epoch, classes, metrics_dict):\n",
    "    meta_learner.proto_net.eval()\n",
    "    meta_learner.classifier.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    all_pred_proba = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(meta_learner.device), y.to(meta_learner.device)\n",
    "            embeddings = meta_learner.proto_net(x)\n",
    "            logits = meta_learner.classifier(embeddings)\n",
    "            loss = F.cross_entropy(logits, y).item()\n",
    "            total_loss += loss * x.size(0)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            pred_proba = torch.softmax(logits, dim=1)\n",
    "            total_correct += (predictions == y).sum().item()\n",
    "            total_samples += y.size(0)\n",
    "            all_targets.extend(y.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_pred_proba.extend(pred_proba.cpu().numpy())\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "    avg_loss = total_loss / total_samples\n",
    "    cm = confusion_matrix(all_targets, all_predictions)\n",
    "    \n",
    "    # 클래스별 성능 보고서 출력\n",
    "    class_report = classification_report(all_targets, all_predictions, target_names=[str(c) for c in classes])\n",
    "    logger.info(f'Classification Report:\\n{class_report}')\n",
    "\n",
    "    all_pred_proba = np.array(all_pred_proba)\n",
    "    all_pred_proba = all_pred_proba / all_pred_proba.sum(axis=1, keepdims=True)\n",
    "\n",
    "    log_performance_metrics(epoch, all_targets, all_predictions, all_pred_proba, avg_loss, classes, metrics_dict)\n",
    "    log_confusion_matrix(cm, classes)\n",
    "\n",
    "    precision = precision_score(all_targets, all_predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_targets, all_predictions, average='weighted', zero_division=0)\n",
    "\n",
    "    print(f'Test set: Average loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}, Recall: {recall:.4f}, Precision: {precision:.4f}')\n",
    "    return avg_loss, accuracy, recall, precision\n",
    "\n",
    "# 전체 테스트 세트에서 모델 평가 (정확도와 리콜을 출력하도록 수정)\n",
    "def evaluate(meta_learner, test_loader, class_labels, epochs, metrics_dict):\n",
    "    meta_learner.proto_net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    all_pred_proba = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device).long()\n",
    "            embeddings = meta_learner.proto_net(data)\n",
    "            output = meta_learner.classifier(embeddings)\n",
    "            test_loss += F.cross_entropy(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_predictions.extend(pred.cpu().numpy().flatten())  # Flatten the predictions\n",
    "            all_pred_proba.extend(torch.softmax(output, dim=1).cpu().numpy())\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    recall = recall_score(all_targets, all_predictions, average='weighted', zero_division=0)\n",
    "    precision = precision_score(all_targets, all_predictions, average='weighted', zero_division=0)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}, Recall: {recall:.4f}, Precision: {precision:.4f}')\n",
    "\n",
    "    # 성능 지표 기록\n",
    "    cm = confusion_matrix(all_targets, all_predictions)\n",
    "\n",
    "    # Ensure y_pred_proba sums to 1 for each sample\n",
    "    all_pred_proba = np.array(all_pred_proba)\n",
    "    all_pred_proba = all_pred_proba / all_pred_proba.sum(axis=1, keepdims=True)\n",
    "\n",
    "    log_performance_metrics(epochs, all_targets, all_predictions, all_pred_proba, test_loss, class_labels, metrics_dict)\n",
    "    log_confusion_matrix(cm, class_labels)\n",
    "    compute_roc_pr_curves(all_targets, all_pred_proba, class_labels)\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train(meta_learner, data_loader, memory_replay, test_loader, epochs=10, replay_frequency=50, support_size=16, n_clusters=5, class_labels=None):\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    test_recalls = []\n",
    "    test_precisions = []\n",
    "    metrics_dict = {}  # 이 부분을 추가하여 metrics_dict 변수를 초기화합니다.\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            support_set, query_set = split_support_query(batch, support_size)\n",
    "            inner_loss = meta_learner.inner_update(support_set)\n",
    "            if i % replay_frequency == 0:\n",
    "                memory = memory_replay.sample_memory()\n",
    "                if memory is not None:\n",
    "                    memory = (memory[0].to(meta_learner.device), memory[1].to(meta_learner.device))\n",
    "                total_loss = meta_learner.outer_update(query_set, memory)\n",
    "            else:\n",
    "                total_loss = meta_learner.outer_update(query_set, None)\n",
    "            \n",
    "            # Calculate training accuracy\n",
    "            embeddings = meta_learner.proto_net(query_set[0].to(meta_learner.device))\n",
    "            logits = meta_learner.classifier(embeddings)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            total_correct += (predictions == query_set[1].to(meta_learner.device)).sum().item()\n",
    "            total_samples += query_set[1].size(0)\n",
    "            \n",
    "            if (i + 1) % 100 == 0:\n",
    "                logger.info(f'Epoch {epoch+1}/{epochs}, Step {i+1}, Loss: {total_loss:.4f}')\n",
    "        \n",
    "        train_accuracy = total_correct / total_samples\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        test_loss, test_accuracy, test_recall, test_precision = test(meta_learner, test_loader, epoch, class_labels, metrics_dict)\n",
    "        train_losses.append(total_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        test_recalls.append(test_recall)\n",
    "        test_precisions.append(test_precision)\n",
    "        logger.info(f'Epoch {epoch+1}/{epochs}, Train Accuracy: {train_accuracy:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# GPU 사용 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device: \", device)\n",
    "\n",
    "# 모델 초기화\n",
    "input_dim = 22572  # 스펙트로그램 특징 차원 (1025 frequency bins, 44 frames)\n",
    "hidden_dim = 128\n",
    "num_classes = 4  # PRPD 데이터셋의 클래스 수\n",
    "\n",
    "prototypical_net = PrototypicalNetwork(input_dim, hidden_dim).to(device)\n",
    "classifier = Classifier(hidden_dim, num_classes).to(device)\n",
    "meta_learner = MetaLearner(prototypical_net, classifier, device=device)\n",
    "\n",
    "# 데이터셋 및 데이터 로더 정의\n",
    "root_dir = '../../../../../Documents/CGB_AI_LAB/Data/PRPD_augmented'  # PRPD 데이터셋의 루트 디렉토리 경로\n",
    "dataset = PRPDDataset(root_dir)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 학습 실행\n",
    "n_classes = 4\n",
    "samples_per_class = 459\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# 테스트할 메모리 사이즈 목록\n",
    "memory_sizes = [200]\n",
    "\n",
    "for memory_size in memory_sizes:\n",
    "    print(f'\\nTesting with memory size: {memory_size}')\n",
    "    \n",
    "    # 정적 메모리 준비\n",
    "    memory_data, memory_labels = [], []\n",
    "    for class_num in range(n_classes):\n",
    "        class_indices = (torch.tensor(train_dataset.dataset.labels) == class_num).nonzero(as_tuple=True)[0][:samples_per_class]\n",
    "        class_subset = Subset(train_dataset.dataset, class_indices)\n",
    "        class_loader = DataLoader(class_subset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        X_class, y_class = next(iter(class_loader))\n",
    "        X_class = X_class.to(device)\n",
    "        y_class = y_class.to(device).long()\n",
    "        \n",
    "        memory_data.append(X_class)\n",
    "        memory_labels.append(y_class)\n",
    "\n",
    "    static_memory_data = torch.cat(memory_data)\n",
    "    static_memory_labels = torch.cat(memory_labels)\n",
    "\n",
    "    memory_replay = StaticMemoryReplay(static_memory_data, static_memory_labels)\n",
    "    class_labels = list(dataset.label_map.keys())\n",
    "    class_labels = [str(label) for label in class_labels]  # Convert class labels to strings\n",
    "\n",
    "    metrics_dict = {}  # Initialize metrics_dict here\n",
    "\n",
    "    for class_num in range(n_classes):\n",
    "        print(f'\\n=== Class {class_num} ===')\n",
    "        \n",
    "        class_indices = (torch.tensor(train_dataset.dataset.labels) == class_num).nonzero(as_tuple=True)[0][:samples_per_class]\n",
    "        class_subset = Subset(train_dataset.dataset, class_indices)\n",
    "        class_loader = DataLoader(class_subset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        X_class, y_class = next(iter(class_loader))\n",
    "        X_class = X_class.to(device)\n",
    "        y_class = y_class.to(device).long()  # Ensure target is of type long\n",
    "        \n",
    "        # 이전 클래스 성능 평가\n",
    "        if class_num > 0:\n",
    "            meta_learner.proto_net.eval()\n",
    "            with torch.no_grad():\n",
    "                previous_accuracy = []\n",
    "                for prev_class_num in range(class_num):\n",
    "                    prev_class_indices = (torch.tensor(train_dataset.dataset.labels) == prev_class_num).nonzero(as_tuple=True)[0][:samples_per_class]\n",
    "                    prev_class_subset = Subset(train_dataset.dataset, prev_class_indices)\n",
    "                    prev_class_loader = DataLoader(prev_class_subset, batch_size=batch_size, shuffle=True)\n",
    "                    \n",
    "                    correct = 0\n",
    "                    total = 0\n",
    "                    for data, target in prev_class_loader:\n",
    "                        data, target = data.to(device), target.to(device)\n",
    "                        embeddings = meta_learner.proto_net(data)\n",
    "                        output = meta_learner.classifier(embeddings)\n",
    "                        pred = output.argmax(dim=1)\n",
    "                        correct += (pred == target).sum().item()\n",
    "                        total += target.size(0)\n",
    "                    \n",
    "                    prev_accuracy = correct / total\n",
    "                    previous_accuracy.append(prev_accuracy)\n",
    "                    print(f'Accuracy on class {prev_class_num}: {prev_accuracy:.4f}')\n",
    "        \n",
    "        X_class_flat = X_class.view(X_class.shape[0], -1).cpu().detach().numpy()\n",
    "        tsne = TSNE(n_components=2, random_state=33)\n",
    "        X_class_tsne = tsne.fit_transform(X_class_flat)\n",
    "\n",
    "        silhouette_scores = []\n",
    "        n_clusters_range = range(2, min(len(X_class_flat), 11))\n",
    "        for n_clusters in n_clusters_range:\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=33, n_init=10).fit(X_class_tsne)\n",
    "            score = silhouette_score(X_class_tsne, kmeans.labels_)\n",
    "            silhouette_scores.append(score)\n",
    "        optimal_n_clusters = np.argmax(silhouette_scores) + 2\n",
    "        kmeans = KMeans(n_clusters=optimal_n_clusters, random_state=33).fit(X_class_tsne)\n",
    "\n",
    "        print(\"Searching for optimal n of clusters...\")\n",
    "        print(f\"Optimal n of clusters for class {class_num}: {optimal_n_clusters} (score: {np.max(silhouette_scores):.4f})\")\n",
    "        print(f\"Set: n_clusters: {optimal_n_clusters}, n_neighbors: {int(memory_size/n_classes/optimal_n_clusters)} (Memory allocated for class {class_num}: {optimal_n_clusters}*{int(memory_size/n_classes/optimal_n_clusters)}={memory_size})\")\n",
    "\n",
    "        n_neighbors = min(int((memory_size/n_classes)/optimal_n_clusters), len(X_class_tsne))\n",
    "        neigh = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "        neigh.fit(X_class_tsne)\n",
    "        memory_data_buffer = []\n",
    "        memory_label_buffer = []\n",
    "        for center in kmeans.cluster_centers_:\n",
    "            _, neighbors = neigh.kneighbors([center], n_neighbors=n_neighbors, return_distance=True)\n",
    "            for neighbor_idx in neighbors[0]:\n",
    "                memory_data_buffer.append(X_class[neighbor_idx].unsqueeze(0))\n",
    "                memory_label_buffer.append(y_class[neighbor_idx].unsqueeze(0))\n",
    "\n",
    "        memory_sample = memory_replay.sample_memory()\n",
    "        if memory_sample:\n",
    "            memory_data = torch.cat((memory_sample[0], torch.cat(memory_data_buffer).to(device)), dim=0)\n",
    "            memory_labels = torch.cat((memory_sample[1], torch.cat(memory_label_buffer).to(device)), dim=0)\n",
    "        else:\n",
    "            memory_data = torch.cat(memory_data_buffer).to(device)\n",
    "            memory_labels = torch.cat(memory_label_buffer).to(device)\n",
    "        \n",
    "        if len(memory_data) > memory_size:\n",
    "            memory_data = memory_data[-memory_size:]\n",
    "            memory_labels = memory_labels[-memory_size:]\n",
    "\n",
    "        memory_replay.memory_data = memory_data\n",
    "        memory_replay.memory_labels = memory_labels\n",
    "\n",
    "        combined_data = torch.cat((X_class, memory_data))\n",
    "        combined_labels = torch.cat((y_class, memory_labels))\n",
    "        combined_dataset = TensorDataset(combined_data, combined_labels)\n",
    "        combined_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        meta_learner.proto_net.train()\n",
    "        for epoch in range(epochs):\n",
    "            for batch_idx, (data, target) in enumerate(combined_loader):\n",
    "                data, target = data.to(device), target.to(device).long()\n",
    "                meta_learner.inner_optimizer.zero_grad()\n",
    "                embeddings = meta_learner.proto_net(data)\n",
    "                output = meta_learner.classifier(embeddings)\n",
    "                loss = F.cross_entropy(output, target)\n",
    "                loss.backward()\n",
    "                meta_learner.inner_optimizer.step()\n",
    "\n",
    "            if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "                print(f'Epoch {epoch+1}/{epochs} Loss: {loss.item():.4f}')\n",
    "\n",
    "        if class_num > 0:\n",
    "            meta_learner.proto_net.eval()\n",
    "            with torch.no_grad():\n",
    "                memory = memory_replay.sample_memory()\n",
    "                if memory:\n",
    "                    output_previous = meta_learner.classifier(meta_learner.proto_net(memory[0]))\n",
    "                    loss_previous = F.cross_entropy(output_previous, memory[1]).item()\n",
    "                    accuracy_previous = (output_previous.argmax(dim=1) == memory[1]).float().mean().item()\n",
    "                    print(f'Performance on previous classes after class {class_num}: Loss: {loss_previous:.4f}, Accuracy: {accuracy_previous:.4f}')\n",
    "\n",
    "        # 분리된 테스트 데이터셋을 사용한 중간 평가\n",
    "        test_loss, test_accuracy, test_recall, test_precision = test(meta_learner, test_loader, class_num, class_labels, metrics_dict)\n",
    "        print(f'Intermediate Test set: Average loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}, Recall: {test_recall:.4f}, Precision: {test_precision:.4f}')\n",
    "\n",
    "    # 전체 테스트 세트에서 모델 평가\n",
    "    evaluate(meta_learner, test_loader, class_labels, epochs, metrics_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b989d0-6f9d-480a-bf0a-c4e87d514779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644ad3b8-69bb-4f7a-9123-ea3a27b14066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
